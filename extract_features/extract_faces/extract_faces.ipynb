{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91298ba5-ce32-4e1c-8d74-243598ba1ff4",
   "metadata": {},
   "source": [
    "### This notebook is a walkthrough of how to use extract_faces.py.\n",
    "\n",
    "You will need to have pliers and its face_recognition dependency installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f94e9-89a9-4702-9e0e-13666de3c451",
   "metadata": {},
   "source": [
    "To install, in command line run:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaf0c000-fafe-490a-8b7f-c2c14998d9d6",
   "metadata": {},
   "source": [
    "pip install pliers\n",
    "pip install face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ac29f-5739-4d0d-83a5-0b0929d2df9c",
   "metadata": {},
   "source": [
    "This script will read a features.json file that defines the frame sampling rate, the download path, and the save path.\n",
    "For convenience, this .json file should also include the other parameters you may need for extracting semantic or low-level visual features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8674e3a-d120-40a2-86be-fcdb70444054",
   "metadata": {},
   "source": [
    "#### Here's an example of what should go in the .json file:\n",
    "\n",
    "\n",
    "{\"hcpmovies\": {  \n",
    "&emsp;      \"hdim\": 90,                ---> desired horizontal dimension of downsampled image  \n",
    "&emsp;      \"vdim\": 128,               ---> desired vertical dimension of downsampled image  \n",
    "&emsp;      \"fps\": 24,                 ---> frames per second of the movie\n",
    "&emsp;      \"dir\": [0, 30, 60,  90, 120, 150, 180, 210, 240, 270, 300, 330], ---> spatial directions of gabors (aka motion direction) \n",
    "&emsp;      \"sf\": [0,4,8,16],               ---> spatial frequency range for gabors  \n",
    "&emsp;      \"tf\": [0,4],               ---> temporal frequency range for gabors  \n",
    "&emsp;      \"samplerate\": 1,           ---> the number of frames per second to sample\n",
    "&emsp;      \"downloadpath\": \"/home/jovyan/shared/hcp-7T_Movies/movie/unzip/Post_20140821_version/\", ---> path to movies  \n",
    "&emsp;      \"movies\": [\"7T_MOVIE1_CC1_v2.mp4\", ---> list of movie names  \n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE2_HO1_v2.mp4\",   \n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE3_CC2_v2.mp4\",   \n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE4_HO2_v2.mp4],\n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;   \n",
    "&emsp; \"savepath\": \"/home/jovyan/workingdirectory/\" ---> where you want to save features  \n",
    "&emsp;      \"TRs\": {\"7T_MOVIE1_CC1_v2\": {\"train1\": [20,265], \"train2\": [285,506], \"train3\": [526,714], \"train4\": [735,798], \"test1\":[818,901]}, ---> list of movie names and the associated ranges of TRs to train and test on\n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE2_HO1_v2\": {\"train5\":[20,248],\"train6\":[267,526],\"train7\":[545,795],\"test2\":[815,898]},____  \n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE3_CC2_v2\": {\"train8\":[20,200],\"train9\": [220,405], \"train10\": [425,628], \"train11\": [650,793], \"test3\": [812,895]},____   \n",
    "&emsp; &emsp; &emsp; &emsp; &emsp;  \"7T_MOVIE4_HO2_v2\": {\"train12\":[20,254],\"train13\":[272,503],\"train14\":[522,777],\"test4\":[798,881]}\n",
    "&emsp; }  \n",
    "}  \n",
    "\n",
    "{\"hcpmovies\": {\n",
    "     \"hdim\": 90,\n",
    "     \"vdim\": 128,\n",
    "     \"fps\": 24,\n",
    "     \"dir\": [0, 30, 60,  90, 120, 150, 180, 210, 240, 270, 300, 330],\n",
    "     \"sf\": [0,4,8,16],\n",
    "     \"tf\": [0,4],\n",
    "     \"samplerate\": 1,\n",
    "     \"downloadpath\": \"/home/jovyan/shared/hcp-7T_Movies/movie/unzip/Post_20140821_version/\",\n",
    "     \"movies\": [\"7T_MOVIE1_CC1_v2.mp4\", \"7T_MOVIE2_HO1_v2.mp4\", \"7T_MOVIE3_CC2_v2.mp4\", \"7T_MOVIE4_HO2_v2.mp4\"],\n",
    "     \"savepath\": \"/home/jovyan/workingdirectory/\",\n",
    "     \"TRs\": {\"7T_MOVIE1_CC1_v2\": {\"train1\": [20,265], \"train2\": [285,506], \"train3\": [526,714], \"train4\": [735,798], \"test1\":[818,901]},\n",
    " \t    \"7T_MOVIE2_HO1_v2\": {\"train5\":[20,248],\"train6\":[267,526],\"train7\":[545,795],\"test2\":[815,898]},\n",
    " \t    \"7T_MOVIE3_CC2_v2\": {\"train8\":[20,200],\"train9\": [220,405], \"train10\": [425,628], \"train11\": [650,793], \"test3\": [812,895]},\n",
    " \t    \"7T_MOVIE4_HO2_v2\": {\"train12\":[20,254],\"train13\":[272,503],\"train14\":[522,777],\"test4\":[798,881]}\n",
    "    \t     }\n",
    "  \t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539c50a8-2049-471c-b54b-30e56a4de05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 16:30:39.083746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import face_recognition\n",
    "import pliers\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from pliers.stimuli import VideoStim\n",
    "from pliers.graph import Graph\n",
    "from pliers.filters import FrameSamplingFilter\n",
    "#from pliers.extractors import (FaceRecognitionFaceLocationsExtractor,\n",
    "                               #FaceRecognitionFaceEncodingsExtractor,\n",
    "                               #MicrosoftAPIFaceExtractor,\n",
    "                               #GoogleVisionAPIFaceExtractor,\n",
    "                               #merge_results)\n",
    "from pliers.extractors import (FaceRecognitionFaceLocationsExtractor,\n",
    "                               FaceRecognitionFaceEncodingsExtractor,\n",
    "                               merge_results)\n",
    "\n",
    "from pliers.converters import VideoToAudioConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471402a3-bf8b-45b6-b2b1-88bdb7b852a0",
   "metadata": {},
   "source": [
    "In python notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96402d1c-dcf9-42f5-8cf2-d81d2b88b4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d50d894-f3ab-4d0e-8195-482967e35895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_filepath = '/home/jovyan/hackathon/visual-feature-decoding/extract_features/feature_AHH.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ec9840-b10e-457c-967d-64ce5b1c7d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'face_recognition' from '/srv/conda/envs/notebook/lib/python3.10/site-packages/face_recognition/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pliers.utils.attempt_to_import('face_recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a73cc2-f49b-48f8-8a74-1ab52e4f38c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stim: 921it [08:52,  1.73it/s]\n",
      "Stim: 918it [08:44,  1.75it/s]\n",
      "Stim: 915it [08:37,  1.77it/s]\n",
      "Stim: 901it [08:43,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "utils.extract_faces(json_filepath)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69b07652-a031-4e4d-a5bf-bbfa1786625a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "975dc957-92b5-44c8-8157-e0fb38121008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan 1.0 82.0 0 (204, 760, 590, 375)]\n",
      " [nan 1.0 84.0 0 (134, 812, 455, 491)]\n",
      " [nan 1.0 85.0 0 (76, 803, 461, 418)]\n",
      " ...\n",
      " [nan 1.0 895.0 2 (147, 250, 199, 199)]\n",
      " [nan 1.0 895.0 3 (155, 602, 229, 527)]\n",
      " [nan 1.0 895.0 4 (87, 467, 149, 405)]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load('/home/jovyan/hackathon/visual-feature-decoding/extract_features/extract_faces/extracted_data/7T_MOVIE1_CC1_v2_faces.npz', allow_pickle=True)\n",
    "\n",
    "# Access the arrays stored in the .npz file using keys\n",
    "features = data['features']\n",
    "print(features)\n",
    "\n",
    "# Close the file after using it\n",
    "data.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
